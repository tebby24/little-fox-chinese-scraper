{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "GENERIC_XML_URL = \"https://cdn.littlefox.co.kr/cn/captionxml/C0001143.xml\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Series:\n",
    "    def __init__(self, series_id):\n",
    "        self.main_url = f'https://chinese.littlefox.com/en/story/contents_list/{series_id}'\n",
    "\n",
    "    def get_page_count(self):\n",
    "        response = requests.get(self.main_url)\n",
    "        html_content = response.text\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        paging_div = soup.find('div', class_='lf_paging')\n",
    "\n",
    "        if paging_div:\n",
    "            page_numbers = []\n",
    "            for a_tag in paging_div.find_all('a'):\n",
    "                try:\n",
    "                    page_number = int(a_tag.text)\n",
    "                    page_numbers.append(page_number)\n",
    "                except ValueError:\n",
    "                    # Ignore non-numeric values\n",
    "                    continue\n",
    "            if page_numbers:\n",
    "                max_page_number = max(page_numbers)\n",
    "                return max_page_number\n",
    "        return None\n",
    "\n",
    "    def get_page_urls(self):\n",
    "        max_page_count = self.get_page_count()\n",
    "        if max_page_count:\n",
    "            page_urls = [f'{self.main_url}?&page={page}' for page in range(1, max_page_count + 1)]\n",
    "            return page_urls\n",
    "        else:\n",
    "            print(\"No pages found\")\n",
    "            return None\n",
    "\n",
    "    def get_ep_ids(self):\n",
    "        page_urls = self.get_page_urls()\n",
    "        print(page_urls)\n",
    "        ids = []\n",
    "        for url in page_urls: \n",
    "            response = requests.get(url)\n",
    "            html_content = response.text\n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "            items = soup.find_all('div', class_='item')\n",
    "            for item in items:\n",
    "                input_element = item.find('input', class_='LF_CHK s2 contentsCheck')\n",
    "                if input_element:\n",
    "                    value = input_element.get('value')\n",
    "                    ids.append(value)\n",
    "        return ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C0008512',\n",
       " 'C0008513',\n",
       " 'C0008514',\n",
       " 'C0008515',\n",
       " 'C0008516',\n",
       " 'C0008517',\n",
       " 'C0008518',\n",
       " 'C0008519',\n",
       " 'C0008520',\n",
       " 'C0008521',\n",
       " 'C0008522',\n",
       " 'C0008523',\n",
       " 'C0008524',\n",
       " 'C0008525',\n",
       " 'C0008526',\n",
       " 'C0008527',\n",
       " 'C0008528',\n",
       " 'C0008529',\n",
       " 'C0008530',\n",
       " 'C0008531',\n",
       " 'C0008532',\n",
       " 'C0008533',\n",
       " 'C0008534',\n",
       " 'C0008535']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_stories_1 = Series('DP000801')\n",
    "single_stories_1.get_ep_ids()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
